import os
import json
import pandas as pd
from tqdm import tqdm
import torch

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.docstore.document import Document
from langchain.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import SentenceTransformer, util

# 1. GPU ì‚¬ìš© ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"âœ… Using device: {device}")

# 2. SBERT ëª¨ë¸ ë¡œë”©
model = SentenceTransformer("jhgan/ko-sbert-nli", device=device)

# 3. ë©”íƒ€ë°ì´í„° ê¸°ì¤€ê°’ ë¡œë”©
with open("../../data/metadata_categories.json", "r", encoding="utf-8") as f:
    metadata_dict = json.load(f)

# 4. ë¬¸ë‹¨ ì¶”ì¶œ í•¨ìˆ˜
def extract_pdf_paragraphs(pdf_path):
    loader = PyMuPDFLoader(pdf_path)
    return loader.load()  # returns List[Document]

# 5. ë¬¸ë‹¨ ë°°ì¹˜ ì„ë² ë”© + ë©”íƒ€ë°ì´í„° íƒœê¹…
def tag_batch_paragraphs(paragraphs, model, metadata_dict, threshold=0.5):
    embeddings = model.encode(paragraphs, batch_size=32, show_progress_bar=True, convert_to_tensor=True)
    results = []

    for para_emb in embeddings:
        tags = {}
        for key, candidates in metadata_dict.items():
            if not candidates:
                continue
            candidate_embs = model.encode(candidates, convert_to_tensor=True)
            scores = util.cos_sim(para_emb, candidate_embs)[0]
            best_idx = scores.argmax().item()
            best_score = scores[best_idx].item()
            if best_score > threshold:
                tags[key] = candidates[best_idx]
        results.append(tags)
    return results

# 6. PDF ë¬¸ì„œ ì²˜ë¦¬
pdf_folder = "../../data/pdf"
results = []
chroma_docs = []

for filename in tqdm(os.listdir(pdf_folder), desc="ğŸ“ PDF ì²˜ë¦¬ ì¤‘"):
    if filename.endswith(".pdf"):
        file_path = os.path.join(pdf_folder, filename)
        print(f"\nğŸ“„ Processing: {filename}")
        documents = extract_pdf_paragraphs(file_path)

        paragraphs = [doc.page_content for doc in documents]
        metadatas = [doc.metadata for doc in documents]

        print("ğŸ“„ ë¬¸ë‹¨ ìˆ˜:", len(paragraphs))
        if len(paragraphs) == 0:
            print("âš ï¸ ë¬¸ë‹¨ì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        tagged_list = tag_batch_paragraphs(paragraphs, model, metadata_dict)

        if len(tagged_list) != len(paragraphs):
            print("âš ï¸ ë¬¸ë‹¨ê³¼ íƒœê¹… ìˆ˜ ë¶ˆì¼ì¹˜. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            continue

        for para, meta, tags in zip(paragraphs, metadatas, tagged_list):
            if not para.strip():
                continue

            # CSV ì €ì¥ìš©
            results.append({
                "pdf_file": filename,
                "paragraph": para,
                "tags": tags,
                "metadata": meta
            })

            # Chroma ì €ì¥ìš©
            combined_meta = meta or {}
            combined_meta.update(tags)
            combined_meta["pdf_file"] = filename

            chroma_docs.append(Document(
                page_content=para,
                metadata=combined_meta
            ))

print(f"\nâœ… ìµœì¢… ì €ì¥í•  ë¬¸ì„œ ìˆ˜: {len(chroma_docs)}ê°œ")

# 7. CSV ì €ì¥
df_results = pd.DataFrame(results)
df_results.to_csv("tagged_pdf_paragraphs.csv", index=False)
print("ğŸ“„ ì™„ë£Œ: tagged_pdf_paragraphs.csv ì €ì¥ë¨")

# 8. Chroma DB ì €ì¥
if len(chroma_docs) == 0:
    print("âŒ ì €ì¥í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Chroma DB ì €ì¥ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
else:
    embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sbert-nli")
    chroma_path = "chroma_construction_db_JKL"

    vectorstore = Chroma.from_documents(
        documents=chroma_docs,
        embedding=embedding_model,
        persist_directory=chroma_path
    )
    vectorstore.persist()
    print(f"âœ… ì™„ë£Œ: Chroma DB ì €ì¥ë¨ â†’ {chroma_path}")
